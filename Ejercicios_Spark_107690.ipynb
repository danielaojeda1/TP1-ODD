{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TRABAJO PRÁCTICO 1 - SPARK\n",
        " - Alumno: Daniela Ojeda.\n",
        " - Padrón: 107690.\n",
        " - Cuatrimestre: 1C2023."
      ],
      "metadata": {
        "id": "ja2WDeozvsnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntGsLQfhwUFY",
        "outputId": "ecbab9f2-4f14-4346-dfb1-1314b273272e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt update\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "#!apt install default-jre\n",
        "#!apt install default-jdk+\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2uSXzS7wBku",
        "outputId": "b5068860-f6b4-4344-a36c-e3c61bfcdcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=a6a059b41d64ebf00d07a088cd0927a061399fd3ce2e2bbf9d424bc11833cf30\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,045 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,157 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,341 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,674 kB]\n",
            "Fetched 8,556 kB in 5s (1,826 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "26 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "id": "QoscaXFBwCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "rsVGlzLjwM7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EJERCICIO 9"
      ],
      "metadata": {
        "id": "YHeOqtbBvwU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Mostrar de forma eficiente el tercer trigrama que tiene mayor frecuencia en los títulos de los contenidos de la wikipedia.'''\n",
        "downloaded = drive.CreateFile({'id':\"1h9Lrqmc4uAkXeC0DJAg3f4VlJLut5C7S\"})\n",
        "downloaded.GetContentFile('contents.csv')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "df_contents = sqlContext.read.csv('/content/drive/MyDrive/orga/wikipedia/contents.csv', header=True, inferSchema=True)\n",
        "rdd_contents = df_contents.rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDTS6wrMvsSP",
        "outputId": "23302c1b-ad9f-49a6-c479-9dd4b64d9811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_filtro = rdd_contents.filter(lambda x: x.title is not None and len(x.title) > 2)\n",
        "rdd_separo_palabras = rdd_filtro.flatMap(lambda x: x[0].lower().split()).filter(lambda x: len(x) > 2)\n",
        "rdd_separo_trigramas = rdd_separo_palabras.flatMap(lambda x: [x[i:i+3] for i in range(len(x) -2)]).filter(lambda x: x.isalpha() and len(x) == 3)\n",
        "rdd_obtengo_frecuencias = rdd_separo_trigramas.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)\n",
        "rdd_top_3_trigramas = rdd_obtengo_frecuencias.takeOrdered(3,key=lambda x: -x[1])\n",
        "tercer_trigrama = rdd_top_3_trigramas[2]\n",
        "print(f'El tercer trigrama que tiene mayor frecuencia en los títulos de los contenidos de la wikipedia es: {tercer_trigrama[0]} con {tercer_trigrama[1]} apariciones')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66DC9pzFwgTN",
        "outputId": "0063cdae-e19b-4c36-c220-91a9261763cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tercer trigrama que tiene mayor frecuencia en los títulos de los contenidos de la wikipedia es: ría con 483092 apariciones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EJERCICIO 19"
      ],
      "metadata": {
        "id": "Fj6AuVQPv5Qf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik-YYqbIvYDL"
      },
      "outputs": [],
      "source": [
        "'''Cantidad de contenido por planeta fuera de la tierra en la Wikipedia.'''\n",
        "downloaded = drive.CreateFile({'id':\"1h9Lrqmc4uAkXeC0DJAg3f4VlJLut5C7S\"})\n",
        "downloaded.GetContentFile('geo_tags.csv')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "df_geo_tags = sqlContext.read.csv('/content/drive/MyDrive/orga/wikipedia/geo_tags.csv', header=True, inferSchema=True)\n",
        "rdd_geo_tags = df_geo_tags.rdd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contenido_fuera_tierra = rdd_geo_tags.filter(lambda x: x.gt_globe != 'earth' and x.gt_globe is not None).map(lambda x: (x.gt_globe, 1)).reduceByKey(lambda a,b: a+b).collect()\n",
        "print(f'La cantidad de contenido por planeta fuera de la Tierra en la Wikipedia es: ')\n",
        "for i in contenido_fuera_tierra:\n",
        "  print(f'• {i[0]} - {i[1]} contenidos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmTQwEpGwnSN",
        "outputId": "4d767f02-a025-40ef-e53d-c46395f35b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La cantidad de contenido por planeta fuera de la Tierra en la Wikipedia es: \n",
            "• mars - 1438 contenidos\n",
            "• titan - 4 contenidos\n",
            "• callisto - 3 contenidos\n",
            "• titania - 2 contenidos\n",
            "• hyperion - 1 contenidos\n",
            "• tethys - 1 contenidos\n",
            "• mimas - 1 contenidos\n",
            "• mercury - 561 contenidos\n",
            "• moon - 9061 contenidos\n",
            "• ganymede - 116 contenidos\n",
            "• venus - 24 contenidos\n",
            "• umbriel - 1 contenidos\n",
            "• phobos - 20 contenidos\n",
            "• oberon - 1 contenidos\n",
            "• io - 2 contenidos\n",
            "• deimos - 2 contenidos\n",
            "• enceladus - 1 contenidos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EJERCICIO 22"
      ],
      "metadata": {
        "id": "ibl4q4icv8rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Top 5 de lenguajes que son usados por usuarios bilingües'''\n",
        "downloaded = drive.CreateFile({'id':\"1h9Lrqmc4uAkXeC0DJAg3f4VlJLut5C7S\"})\n",
        "downloaded.GetContentFile('languages.csv')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "df_languages = sqlContext.read.csv('/content/drive/MyDrive/orga/wikipedia/languages.csv', header=True, inferSchema=True)\n",
        "rdd_languages = df_languages.rdd"
      ],
      "metadata": {
        "id": "BOQ-5n5zv-tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_languages_new = rdd_languages.map(lambda x: (x.babel_user, x.babel_lang, '4.5') if x.babel_level == 'N' else x)\n",
        "rdd_languages_pasaje_int = rdd_languages_new.map(lambda x: (x[0], x[1], float(x[2])))\n",
        "\n",
        "rdd_languages_bilingues_idiomas = rdd_languages_pasaje_int.filter(lambda x: x[2] >= 1).map(lambda x: (x[0], [x[1].lower()])).reduceByKey(lambda a,b: a + b).filter(lambda x: len(x[1]) > 1)\n",
        "rdd_languages_contador_idiomas = rdd_languages_bilingues_idiomas.flatMap(lambda x: [(idioma, 1) for idioma in set(x[1])]).reduceByKey(lambda a,b: a + b)\n",
        "top_5_usados = rdd_languages_contador_idiomas.takeOrdered(5, key=lambda x: -x[1])\n",
        "print('Los 5 idiomas más usados por usuarios bilingües son:')\n",
        "for i in top_5_usados:\n",
        "  print(f'• {i[0]} ({i[1]} usuarios)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlcXI7cjw7yV",
        "outputId": "7cdd13d1-90a6-4dff-f1bd-ce9858766d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los 5 idiomas más usados por usuarios bilingües son:\n",
            "• en (9318 usuarios)\n",
            "• es (9200 usuarios)\n",
            "• fr (3476 usuarios)\n",
            "• it (1738 usuarios)\n",
            "• de (1642 usuarios)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EJERCICIO 24"
      ],
      "metadata": {
        "id": "ZBXPoPMIv-W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Dado un tamaño de vocabulario parametrizable y una lista de stopwords también parametrizable implemente tf-IDF para los textos de los contenidos de\n",
        "forma distribuida. Debe obtener un vector por cada texto'''\n",
        "downloaded = drive.CreateFile({'id':\"1h9Lrqmc4uAkXeC0DJAg3f4VlJLut5C7S\"})\n",
        "downloaded.GetContentFile('contents_text_sample.csv')\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "df_contents_text_sample = sqlContext.read.csv('/content/drive/MyDrive/orga/wikipedia/contents_text_sample.csv', header=True, inferSchema=True)\n",
        "rdd_contents_text_sample = df_contents_text_sample.rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRQwQZWXyHWF",
        "outputId": "f45a25cb-a53f-47ee-9587-903970e2801e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Ya se que este ejercicio no esta bien hecho pero me costo mucho plantearlo y preferi entregar algo aunque no funcione, antes que no entregar nada\n",
        "#Uso esto de ejemplo a ver si funciona\n",
        "#stopwordsEsp = stopwords.words('spanish')\n",
        "#vocabulario_size = 10\n",
        "\n",
        "def tokenizar(texto, stopwords):\n",
        "  tokens = word_tokenize(texto.lower())\n",
        "  tokens = [token for token in tokens if token.isalnum() and token not in stopwords]\n",
        "  return tokens\n",
        "\n",
        "rdd_contents_text_sample = rdd_contents_text_sample.filter(lambda x: x[2] is not None).map(lambda x: (x[1],tokenizar(x[2], stopwordsEsp)))\n",
        "rdd_textos = rdd_contents_text_sample.flatMap(lambda x: x[1]).distinct().take(vocabulario_size)\n",
        "\n",
        "def calculo_tf(termino, tokens):\n",
        "  return len(tokens.filter(termino))/len(tokens)\n",
        "\n",
        "def contienen_termino(termino, rdd):\n",
        "  return len(rdd.map(lambda x: tokenizar(x[2], stopwordsEsp)).filter(lambda x: termino in x))\n",
        "\n",
        "def calculo_idf(termino, rdd):\n",
        "  return math.log(len(rdd.text)/(1+contienen_termino(termino,rdd)))\n",
        "\n",
        "def formar_vector(tokens, rdd, rdd2):\n",
        "  vector = []\n",
        "  for termino in rdd2:\n",
        "    tf = calculo_tf(termino, tokens)\n",
        "    idf = calculo_idf(termino,rdd)\n",
        "    vector.append(tf*idf)\n",
        "  return vector\n",
        "\n",
        "rdd_vectores = rdd_contents_text_sample.map(lambda x: (x[0], formar_vector(x[1], rdd_contents_text_sample, rdd_textos)))\n",
        "rdd_vectores.take(5)"
      ],
      "metadata": {
        "id": "ur4zGzwv0NKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}